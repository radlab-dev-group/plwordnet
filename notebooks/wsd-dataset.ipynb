{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": "wsd_dataset_path = \"/mnt/data2/data/datasets/wsd\"",
   "id": "9e071b92ba63834e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "811bae4e15cbc5d2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Iterator, Dict, List\n",
    "\n",
    "\n",
    "class JsonlFileReader:\n",
    "    \"\"\"\n",
    "    Iterator over a single JSON Lines file.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    file_path : str or Path\n",
    "        Path to the `.jsonl` file to read.\n",
    "\n",
    "    Usage\n",
    "    -----\n",
    "    >>> reader = JsonlFileReader(\"example.jsonl\")\n",
    "    >>> for record in reader:\n",
    "    ...     print(record)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, file_path: str | os.PathLike):\n",
    "        self.file_path = Path(file_path)\n",
    "        if not self.file_path.is_file():\n",
    "            raise FileNotFoundError(f\"File not found: {self.file_path}\")\n",
    "\n",
    "    def __iter__(self) -> Iterator[Dict]:\n",
    "        \"\"\"Yield one parsed JSON dict per line.\"\"\"\n",
    "        with self.file_path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if line:  # skip empty lines\n",
    "                    try:\n",
    "                        yield json.loads(line)\n",
    "                    except json.JSONDecodeError as exc:\n",
    "                        raise ValueError(\n",
    "                            f\"Invalid JSON in {self.file_path} on line {f.tell()}\"\n",
    "                        ) from exc\n",
    "\n",
    "\n",
    "def list_jsonl_files(directory: str | os.PathLike) -> List[Path]:\n",
    "    \"\"\"Return a list of all `.jsonl` files in the given directory.\"\"\"\n",
    "    p = Path(directory)\n",
    "    if not p.is_dir():\n",
    "        raise NotADirectoryError(f\"Directory not found: {p}\")\n",
    "    return list(p.glob(\"*.jsonl\"))"
   ],
   "id": "cebe038f0a6bd5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "c460b58ea144391d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "9df741bfcf0aeb0d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "f5eb9a39d5b72698",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "c23bb670defa6422",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "b913cbaafa277487"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-20T16:05:58.053952Z",
     "start_time": "2025-08-20T16:05:58.037369Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# Updated Text class – defensive against missing keys in the JSONL record\n",
    "# --------------------------------------------------------------------------- #\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Any, Dict, Iterable, List\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class AnnotationLayer:\n",
    "    items: List[Dict[str, Any]] = field(default_factory=list)\n",
    "\n",
    "    def __iter__(self) -> Iterable[Dict[str, Any]]:\n",
    "        return iter(self.items)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.items)\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"{self.__class__.__name__}({len(self.items)} items)\"\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TokensLayer(AnnotationLayer):\n",
    "    pass\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class WsdLayer(AnnotationLayer):\n",
    "    pass\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class PhrasesLayer(AnnotationLayer):\n",
    "    pass\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Text:\n",
    "    context_file: str\n",
    "    raw_text: str\n",
    "    tokens: TokensLayer = field(default_factory=TokensLayer)\n",
    "    wsd: WsdLayer = field(default_factory=WsdLayer)\n",
    "    phrases: PhrasesLayer = field(default_factory=PhrasesLayer)\n",
    "\n",
    "    @classmethod\n",
    "    def from_dict(cls, d: Dict[str, Any]) -> \"Text\":\n",
    "        \"\"\"\n",
    "        Build a Text instance from a dictionary produced by a JSONL line.\n",
    "        All keys are fetched with .get() to avoid KeyError if a field\n",
    "        is missing in some records.\n",
    "        \"\"\"\n",
    "        return cls(\n",
    "            context_file=d.get(\"context_file\", \"unknown\"),\n",
    "            raw_text=d.get(\"text\", \"\"),\n",
    "            tokens=TokensLayer(d.get(\"tokens\", [])),\n",
    "            wsd=WsdLayer(d.get(\"wsd\", [])),\n",
    "            phrases=PhrasesLayer(d.get(\"phrases\", [])),\n",
    "        )\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return (\n",
    "            f\"Text(file={self.context_file!r}, len={len(self.raw_text)} \"\n",
    "            f\"tokens={len(self.tokens)} wsd={len(self.wsd)} phrases={len(self.phrases)})\"\n",
    "        )\n",
    "\n",
    "    def get_annotation(self, index: int) -> Dict[str, Any]:\n",
    "        token_entry = next((t for t in self.tokens if t[\"index\"] == index), None)\n",
    "        if token_entry is None:\n",
    "            raise ValueError(f\"No token with index {index}\")\n",
    "\n",
    "        wsd_entry = next((w for w in self.wsd if w[\"index\"] == index), None)\n",
    "        phrase_entry = next((p for p in self.phrases if p[\"head\"] == index), None)\n",
    "\n",
    "        return {\"token\": token_entry, \"wsd\": wsd_entry, \"phrase\": phrase_entry}\n",
    "\n",
    "    def get_context(self, index: int, window: int = 5) -> Dict[str, Any]:\n",
    "        center_token = next((t for t in self.tokens if t[\"index\"] == index), None)\n",
    "        if center_token is None:\n",
    "            raise ValueError(f\"No token with index {index}\")\n",
    "\n",
    "        center_pos = self.tokens.items.index(center_token)\n",
    "\n",
    "        left_tokens = self.tokens.items[max(0, center_pos - window) : center_pos]\n",
    "        right_tokens = self.tokens.items[center_pos + 1 : center_pos + 1 + window]\n",
    "\n",
    "        full_window = left_tokens + [center_token] + right_tokens\n",
    "        context_text = \" \".join(tok[\"orth\"] for tok in full_window)\n",
    "\n",
    "        return {\n",
    "            \"center\": center_token,\n",
    "            \"left\": left_tokens,\n",
    "            \"right\": right_tokens,\n",
    "            \"text\": context_text,\n",
    "        }\n",
    "\n",
    "    def annotate_tokens_with_context(self, window: int = 5) -> List[Dict[str, Any]]:\n",
    "        return [\n",
    "            self.get_annotation(tok[\"index\"])\n",
    "            | self.get_context(tok[\"index\"], window)\n",
    "            for tok in self.tokens\n",
    "        ]"
   ],
   "id": "442d0f6d7671601f",
   "outputs": [],
   "execution_count": 52
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Demo: list all jsonl files in the dataset directory\n",
    "jsonl_files = list_jsonl_files(wsd_dataset_path)\n",
    "print(f\"Found {len(jsonl_files)} JSONL files in {wsd_dataset_path}:\")\n",
    "for i, fp in enumerate(jsonl_files, 1):\n",
    "    print(f\"{i}. {fp.name}\")\n",
    "\n",
    "# If there is at least one file, open it and show the first 5 records\n",
    "if jsonl_files:\n",
    "    sample_file = jsonl_files[3]\n",
    "    print(f\"\\nShowing first 5 records from {sample_file.name}:\")\n",
    "    reader = JsonlFileReader(sample_file)\n",
    "    for idx, record in enumerate(reader):\n",
    "        if idx >= 1:\n",
    "            break\n",
    "        print(list(record.keys()))\n",
    "        print(record)\n",
    "        # print(f\"Record {idx+1}:\", json.dumps(record[\"wsd\"], indent=2, ensure_ascii=False))"
   ],
   "id": "6c01a360c962056b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Example – build a Text from the first record of the first JSONL file\n",
    "if jsonl_files:\n",
    "    # Get an iterator over the file\n",
    "    reader_iter = iter(JsonlFileReader(jsonl_files[0]))\n",
    "    first_record = next(reader_iter)  # <-- fixed\n",
    "    sample_text = Text.from_dict(first_record)\n",
    "    print(sample_text)\n",
    "    print(\"First 3 tokens:\", list(sample_text.tokens)[:3])\n",
    "    print(\"First 3 WSD entries:\", list(sample_text.wsd)[:3])\n",
    "    print(\"First 3 phrases:\", list(sample_text.phrases)[:3])\n",
    "else:\n",
    "    print(\"No JSONL files found in\", wsd_dataset_path)"
   ],
   "id": "9c70b1aba831b927",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-20T16:00:57.492400Z",
     "start_time": "2025-08-20T16:00:57.467823Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if jsonl_files:\n",
    "    reader_iter = iter(JsonlFileReader(jsonl_files[1]))\n",
    "    first_record = next(reader_iter)\n",
    "    sample_text = Text.from_dict(first_record)\n",
    "\n",
    "    # 1. Annotation for token index 50 + surrounding context\n",
    "    ann_50 = sample_text.get_annotation(50)\n",
    "    ctx_50 = sample_text.get_context(50, window=5)\n",
    "\n",
    "    print(\"Token 50 annotation:\")\n",
    "    print(ann_50)\n",
    "    print(\"\\nContext around token 50:\")\n",
    "    print(ctx_50[\"text\"])\n",
    "    print(\"\\nLeft tokens:\", [t[\"orth\"] for t in ctx_50[\"left\"]])\n",
    "    print(\"Right tokens:\", [t[\"orth\"] for t in ctx_50[\"right\"]])\n",
    "\n",
    "    # 2. All tokens annotated with context (slower but convenient)\n",
    "    full_ann = sample_text.annotate_tokens_with_context(window=3)\n",
    "    print(\"\\nFirst annotated token with context:\")\n",
    "    print(full_ann[0])"
   ],
   "id": "ab644683bd467e85",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token 50 annotation:\n",
      "{'token': {'index': 50, 'position': [257, 263], 'orth': 'jednym', 'lemma': 'jeden', 'pos': 'adj', 'ctag': 'adj:loc:pos:n:sg'}, 'wsd': None, 'phrase': None}\n",
      "\n",
      "Context around token 50:\n",
      "Chciał by m tylko o jednym powiedzieć , że o ile\n",
      "\n",
      "Left tokens: ['Chciał', 'by', 'm', 'tylko', 'o']\n",
      "Right tokens: ['powiedzieć', ',', 'że', 'o', 'ile']\n",
      "\n",
      "First annotated token with context:\n",
      "{'token': {'index': 0, 'position': [0, 5], 'orth': 'Poseł', 'lemma': 'poseł', 'pos': 'noun', 'ctag': 'subst:nom:m1:sg'}, 'wsd': {'index': 0, 'pl_sense': 'poseł.2.n', 'plWN_syn_id': 'f3bcc549-aac4-11ed-aae5-0242ac130002', 'plWN_lex_id': 'd49dde9d-aac4-11ed-aae5-0242ac130002', 'plWN_syn_legacy_id': '6360', 'plWN_lex_legacy_id': '6084', 'PWN_syn_id': '10522035-n', 'bn_syn_id': 'bn:00067199n', 'mapping_relation': 'hypernymy'}, 'phrase': None, 'center': {'index': 0, 'position': [0, 5], 'orth': 'Poseł', 'lemma': 'poseł', 'pos': 'noun', 'ctag': 'subst:nom:m1:sg'}, 'left': [], 'right': [{'index': 1, 'position': [6, 11], 'orth': 'Jerzy', 'lemma': 'Jerzy', 'pos': 'noun', 'ctag': 'subst:nom:m1:sg'}, {'index': 2, 'position': [12, 19], 'orth': 'Kozdroń', 'lemma': 'kozdroń', 'pos': 'noun', 'ctag': 'subst:nom:m1:sg'}, {'index': 3, 'position': [19, 20], 'orth': ':', 'lemma': ':', 'pos': 'interp', 'ctag': 'interp'}], 'text': 'Poseł Jerzy Kozdroń :'}\n"
     ]
    }
   ],
   "execution_count": 48
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "5bfd7e2757b3dfe5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-20T16:06:10.514604Z",
     "start_time": "2025-08-20T16:06:05.246227Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from collections import Counter\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Helper that collects WSD statistics from a single Text instance\n",
    "def wsd_stats_from_text(text_obj: Text) -> Dict[str, Any]:\n",
    "    senses = [entry[\"pl_sense\"] for entry in text_obj.wsd if \"pl_sense\" in entry]\n",
    "    return {\n",
    "        \"total_wsd\": len(senses),\n",
    "        \"unique_senses\": len(set(senses)),\n",
    "        \"most_common_sense\": (\n",
    "            Counter(senses).most_common(1)[0][0] if senses else None\n",
    "        ),\n",
    "    }\n",
    "\n",
    "\n",
    "# --- Walk through all files and aggregate statistics ---\n",
    "\n",
    "file_stats = []\n",
    "\n",
    "for jsonl_path in jsonl_files:\n",
    "    file_name = os.path.basename(jsonl_path)\n",
    "    # Build a counter for the whole file (multiple records per file)\n",
    "    file_wsd_counter = Counter()\n",
    "    file_total_wsd = 0\n",
    "    file_unique_senses = set()\n",
    "\n",
    "    # Iterate over every record in the file\n",
    "    for record in JsonlFileReader(jsonl_path):\n",
    "        txt = Text.from_dict(record)\n",
    "        stats = wsd_stats_from_text(txt)\n",
    "\n",
    "        file_wsd_counter.update([e[\"pl_sense\"] for e in txt.wsd if \"pl_sense\" in e])\n",
    "        file_total_wsd += stats[\"total_wsd\"]\n",
    "        file_unique_senses.update(\n",
    "            [e[\"pl_sense\"] for e in txt.wsd if \"pl_sense\" in e]\n",
    "        )\n",
    "\n",
    "    most_common = file_wsd_counter.most_common(1)[0][0] if file_wsd_counter else None\n",
    "\n",
    "    file_stats.append(\n",
    "        {\n",
    "            \"file\": file_name,\n",
    "            \"total_wsd\": file_total_wsd,\n",
    "            \"unique_senses\": len(file_unique_senses),\n",
    "            \"most_common_sense\": most_common,\n",
    "        }\n",
    "    )\n",
    "\n",
    "# Convert to DataFrame for nicer printing\n",
    "df_stats = pd.DataFrame(file_stats)\n",
    "df_stats = df_stats.sort_values(\"total_wsd\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "print(\"WSD statistics per file:\")\n",
    "print(df_stats.to_string(index=False))"
   ],
   "id": "6a11ce8ffe128729",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WSD statistics per file:\n",
      "                     file  total_wsd  unique_senses most_common_sense\n",
      "       walenty_text.jsonl      46345          10932        bardzo.1.r\n",
      "     skladnica_text.jsonl      40815          13959           być.3.v\n",
      "       emoglex_text.jsonl      39781          12818           być.9.v\n",
      "      wikiglex_text.jsonl      21079           8506          być.10.v\n",
      "          kpwr_text.jsonl      14387           3128           pan.4.n\n",
      "      kpwr-100_text.jsonl      13397           5678           być.9.v\n",
      "      sherlock_text.jsonl       3769           2049           być.9.v\n",
      " sherlock_sentences.jsonl          0              0              None\n",
      "  emoglex_sentences.jsonl          0              0              None\n",
      "  walenty_sentences.jsonl          0              0              None\n",
      " wikiglex_sentences.jsonl          0              0              None\n",
      " kpwr-100_sentences.jsonl          0              0              None\n",
      "skladnica_sentences.jsonl          0              0              None\n",
      "     kpwr_sentences.jsonl          0              0              None\n"
     ]
    }
   ],
   "execution_count": 53
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
