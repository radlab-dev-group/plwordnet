## INSTALL ‚Äì Kompletny przewodnik budowy zbior√≥w i embeddera dla **S≈Çowosieci**

---

### Spis tre≈õci

1. [Wymagania wstƒôpne](#wymagania-wstƒôpne)
2. [Instalacja bazy S≈Çowosieci](#instalacja-bazy-s≈Çowosieci)
3. [Krok‚ÄØ1 ‚Äì Przygotowanie wag relacji](#krok‚Äë1‚Äëprzygotowanie-wag-relacji)
4. [Krok‚ÄØ2 ‚Äì Budowa grafu artyku≈Ç√≥w Wikipedii](#krok‚Äë2‚Äëbudowa-grafu-artykul√≥w-wikipedii)
5. [Krok‚ÄØ3 ‚Äì Tworzenie zbioru danych embeddera](#krok‚Äë3‚Äëtworzenie-zbioru-danych-embeddera)
6. [Krok‚ÄØ4 ‚Äì Trening embeddera (bi‚Äëencodera)](#krok‚Äë4‚Äëtrening-embeddera-biencodera)
7. [Krok‚ÄØ5 ‚Äì Generowanie embedding√≥w dla znacze≈Ñ](#krok‚Äë5‚Äëgenerowanie-embedding√≥w-dla-znacze≈Ñ)
8. [Krok‚ÄØ6 ‚Äì Eksport danych dla RelGAT](#krok‚Äë6‚Äëeksport-danych-dla-relgat)
9. [Skr√≥cone ≈õcie≈ºki (gotowe artefakty)](#skr√≥t‚Äë≈õcie≈ºki‚Äëgotowe‚Äëartefakty)

---  

## Wymagania wstƒôpne

| Element                | Minimalna wersja | Uwagi                                                                                 |
|------------------------|------------------|---------------------------------------------------------------------------------------|
| **Python**             | 3.10.6           | U≈ºywany wirtualny ≈õrodowisko `virtualenv`                                             |
| **MySQL**              | 5.7+             | Do przechowywania bazy S≈Çowosieci                                                     |
| **Milvus**             | 2.2+             | Baza wektorowa                                                                        |
| **CUDA** (opcjonalnie) | 11+              | Przyspieszenie treningu i inferencji                                                  |
| **Pakiety Python**     | ‚Äì                | Zainstalowane z `requirements.txt` (np. `requests`, `pandas`, `networkx`, `torch`, ‚Ä¶) |

Instalacjƒô pakiet√≥w wykonujemy w aktywowanym ≈õrodowisku:

```shell script
python -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt
```

---  

## Instalacja bazy S≈Çowosieci

1. Pobierz najnowszy dump **S≈Çowosieci** z oficjalnej strony:  
   <http://plwordnet.pwr.wroc.pl/wordnet/download>
2. Utw√≥rz bazƒô MySQL i za≈Çaduj dump:

```shell script
# 1Ô∏è‚É£ Po≈ÇƒÖcz siƒô z serwerem MySQL
mysql -u root -p

# 2Ô∏è‚É£ Utw√≥rz nowƒÖ bazƒô
CREATE DATABASE wordnet_work;

# 3Ô∏è‚É£ Za≈Çaduj plik dumpu (przyk≈Çadowa nazwa)
mysql -u USER -p wordnet_work < wordnet_work_4_5.sql
```

> **Uwaga:** Nazwy u≈ºytkownika oraz has≈Ça dopasuj do w≈Çasnej konfiguracji.

---  

## Krok‚ÄØ1 ‚Äì Przygotowanie wag relacji

Relacje pomiƒôdzy synsetami muszƒÖ mieƒá przypisane wagi, kt√≥re sƒÖ wykorzystywane przy budowie grafu oraz przy trenowaniu
embeddera.

* **Opcja A ‚Äì u≈ºycie gotowego pliku**  
  Repozytorium zawiera aktualny plik `resources/mappings/relation-types-weights-hist.xlsx`. Wystarczy go pozostawiƒá w
  miejscu domy≈õlnym.

* **Opcja B ‚Äì samodzielne przygotowanie**
    1. Utw√≥rz arkusz XLSX ze wszystkimi typami relacji.
    2. W kolumnie `embedder_weight_coarse` wpisz wagi (liczby rzeczywiste).
    3. Uruchom skrypt, kt√≥ry przetworzy arkusz:

```shell script
bash scripts/0-plwordnet-cli-prepare-relations.sh
```

> **Uwaga:** Skrypt mo≈ºna dostosowaƒá, edytujƒÖc zmienne wej≈õciowe.

---  

## Krok‚ÄØ2 ‚Äì Budowa grafu artyku≈Ç√≥w Wikipedii

Graf ≈ÇƒÖczy jednostki leksykalne (LU) i synsety z tekstami Wikipedii.

* **Pe≈Çna budowa od zera** (zalecane, je≈õli chcesz w≈Çasne dane):

```shell script
bash scripts/1-plwordnet-cli-dump-to-nx.sh
```

> Skrypt pobiera dump Wikipedii, przetwarza go na graf `networkx` i zapisuje w katalogu
`resources/plwordnet_4_5/full/graphs/...`.  
> W trakcie przetwarzania wykorzystywany jest lokalny serwis **OpenAPI** do korekcji interpunkcji ‚Äì przy du≈ºej liczbie
> wƒÖtk√≥w mo≈ºe to byƒá czasoch≈Çonne.

* **Alternatywa ‚Äì u≈ºycie gotowego grafu**  
  Je≈õli nie chcesz budowaƒá grafu samodzielnie, zainstaluj pe≈Çne zale≈ºno≈õci aplikacji (`FULL/TEST_GRAPH`) i pomi≈Ñ ten
  krok.

* **RozwiƒÖzywanie problem√≥w**  
  Po zako≈Ñczeniu budowy mo≈ºe pojawiƒá siƒô komunikat `... node has no data ...`. W takim wypadku uruchom ponownie skrypt ‚Äì
  pamiƒôƒá podrƒôczna (`cache`) zapewnia, ≈ºe artyku≈Çy nie bƒôdƒÖ pobierane ponownie.

---  

## Krok‚ÄØ3 ‚Äì Tworzenie zbioru danych embeddera

### 3.1 Zrzut surowych relacji

```shell script
bash scripts/2-plwordnet-cli-dump-embedder-raw.sh
```

* Skrypt generuje plik `.../embedder/plwn_4_5_embedder_raw.jsonl` zawierajƒÖcy pary zda≈Ñ `{zdanie_1, zdanie_2, rel_i}`.
* Wykorzystuje plik wag relacji (z kroku 1).
* Parametr `--embedder-low-high-ratio 2.0` oznacza **2 negatywne przyk≈Çady na ka≈ºdy pozytywny**.

### 3.2 Konwersja do finalnego formatu

```shell script
bash scripts/3-raw-embedder-to-proper-dataset.sh
```

* Tworzy podzielony na `train`/`test` zestaw w formacie `jsonl`.
* Domy≈õlny podzia≈Ç: 90 % trening, 10 % test (`--train-ratio=0.90`).
* Dzieli dane na zdania (`--split-to-sentences`), uruchamia 32 wƒÖtki (`--n-workers=32`) i przetwarza w partiach po 500
  rekord√≥w (`--batch-size=500`).

### 3.3 Deduplication (usuwanie duplikat√≥w)

```shell script
bash scripts/4-deduplicate-embedder-dataset.sh
```

* Usuwa powtarzajƒÖce siƒô rekordy, filtruje przyk≈Çady kr√≥tsze ni≈º 25 znak√≥w oraz zapisuje czysty zbi√≥r gotowy do
  treningu.

> Po wykonaniu powy≈ºszych trzech pod‚Äëskrypt√≥w otrzymujesz kompletny, zbalansowany i oczyszczony dataset do wytrenowania
> embeddera.

---  

## Krok‚ÄØ4 ‚Äì Trening embeddera (bi‚Äëencodera)

Trening odbywa siƒô na przygotowanym zbiorze przy u≈ºyciu modeli **EuroBERT**.

### Dostƒôpne skrypty treningowe

| Skrypt                                  | Model         | Liczba parametr√≥w |
|-----------------------------------------|---------------|-------------------|
| `run_train_biencoder_eurobert_0.61b.sh` | EuroBERT‚Äë610M | 610‚ÄØM             |
| `run_train_biencoder_eurobert_2.1b.sh`  | EuroBERT‚Äë2.1B | 2.1‚ÄØB             |

Przyk≈Çad uruchomienia (model 610 M):

```shell script
bash plwordnet_ml/training_scripts/run_train_biencoder_eurobert_0.61b.sh
```

> **Uwaga:** Trening od zera wymaga kilku dni na typowym GPU. Zdecydowanie szybciej jest pobraƒá gotowe wagi z
> HuggingFace:  
> <https://huggingface.co/radlab/semantic-euro-bert-encoder-v1>

---  

## Krok‚ÄØ5 ‚Äì Generowanie embedding√≥w dla znacze≈Ñ

Po uzyskaniu wytrenowanego (lub pobranego) modelu tworzymy wektorowe reprezentacje dla wszystkich LU i synset√≥w oraz
zapisujemy je w bazie **Milvus**.

### 5.1 Inicjalizacja bazy Milvus

```shell script
plwordnet-milvus \
  --log-level=DEBUG \
  --milvus-config=resources/configs/milvus-config.json \
  --prepare-database
```

### 5.2 Przygotowanie embedding√≥w (real + fake)

```shell script
plwordnet-milvus \
  --milvus-config=resources/configs/milvus-config.json \
  --embedder-config=resources/configs/embedder-config.json \
  --nx-graph-dir="resources/plwordnet_4_5/full/graphs/full/nx/graphs/" \
  --device="cuda:1" \
  --log-level=INFO \
  --prepare-base-embeddings-lu \
  --prepare-base-embeddings-synset \
  --prepare-base-mean-empty-embeddings-lu
```

* **`--prepare-base-embeddings-lu`** ‚Äì wylicza embeddingi dla jednostek leksykalnych.
* **`--prepare-base-embeddings-synset`** ‚Äì wylicza embeddingi dla synset√≥w.
* **`--prepare-base-mean-empty-embeddings-lu`** ‚Äì tworzy tzw. *fake* embeddingi (≈õrednie wektory) wykorzystywane przy
  brakujƒÖcych danych.

Po uruchomieniu zobaczysz log podobny do:

```
2025-10-19 15:42:10,922 - plwordnet_handler.base.connectors.milvus.initializer - INFO - initializer.py:73- Connected to default Milvus database at 192.168.100.67:19533
...
2025-10-19 15:42:21,612 - plwordnet_handler.base.connectors.milvus.initializer - INFO - initializer.py:162- Milvus WordNet handler initialized successfully
```

---  

## Krok‚ÄØ6 ‚Äì Eksport danych dla RelGAT

Aby wytrenowaƒá model RelGAT potrzebny jest specjalny zestaw danych (mapping + same przyk≈Çady).

```shell script
plwordnet-milvus \
  --milvus-config=resources/configs/milvus-config-pk.json \
  --nx-graph-dir="resources/plwordnet_4_5/full/graphs/full/nx/graphs/" \
  --relgat-mapping-directory="resources/plwordnet_4_5/full/relgat/aligned-dataset-identifiers/o78zalgm" \
  --relgat-dataset-directory="resources/plwordnet_4_5/full/relgat/aligned-dataset-identifiers/o78zalgm/dataset_syn_two_way" \
  --log-level=DEBUG \
  --export-relgat-dataset \
  --export-relgat-mapping
```

* **`--export-relgat-dataset`** ‚Äì zapisuje gotowy zestaw treningowy w formacie wymaganym przez RelGAT.
* **`--export-relgat-mapping`** ‚Äì generuje plik mapujƒÖcy identyfikatory w grafie na identyfikatory u≈ºywane w modelu.

---  

## Skr√≥t ≈õcie≈ºki ‚Äì Gotowe artefakty

| Cel                                     | Skrypt / komenda                                                     | Co otrzymasz                                                             |
|-----------------------------------------|----------------------------------------------------------------------|--------------------------------------------------------------------------|
| Gotowy graf z Wikipedii                 | `bash scripts/1-plwordnet-cli-dump-to-nx.sh` (lub pobranie gotowego) | `resources/plwordnet_4_5/full/graphs/...`                                |
| Gotowy plik wag relacji                 | `resources/mappings/relation-types-weights-hist.xlsx`                | Gotowy arkusz Excel                                                      |
| Gotowy dataset embeddera (train‚ÄØ+‚ÄØtest) | `bash scripts/4-deduplicate-embedder-dataset.sh`                     | `resources/plwordnet_4_5/full/embedder/plwn_4_5_embedder_dataset/*.json` |
| Wytrenowane modele                      | Pobranie z HuggingFace **lub** uruchomienie `run_train_biencoder_*`  | `OUT_DIR/.../biencoder/<timestamp>_...`                                  |
| Pe≈Çna inicjalizacja Milvus + embeddingi | `bash scripts/6-plwordnet-milvus-full-init.sh`                       | Baza Milvus gotowa do zapyta≈Ñ                                            |
| Eksport RelGAT                          | `bash scripts/7-plwordnet-milvus-relgat-export.sh`                   | `resources/.../relgat/...`                                               |

---

### Dodatkowe uwagi

* Wszystkie skrypty znajdujƒÖ siƒô w katalogu `scripts/`.
* Je≈õli chcesz u≈ºywaƒá w≈Çasnych konfiguracji (np. inny host Milvus, inny model BERT), edytuj odpowiednie pliki w
  `resources/configs/`.
* W razie problem√≥w z zale≈ºno≈õciami systemowymi (np. brak `libmysqlclient`), sprawd≈∫ sekcjƒô **Issues** w repozytorium
  lub otw√≥rz nowy ticket.

---  

> **Gotowe!** Po przej≈õciu powy≈ºszych krok√≥w masz kompletny ekosystem: baza danych, graf, zbi√≥r treningowy, wytrenowany
> embedder oraz gotowe wektory w Milvus, gotowe do dalszych eksperyment√≥w (np. wyszukiwanie semantyczne, klasyfikacja
> relacji, model RelGAT).

*Powodzenia!* üöÄ  